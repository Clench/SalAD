{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from csv import reader\n",
    "from os import listdir, makedirs, path\n",
    "from pickle import dump\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from model import Seq2Seq, RecurrentAutoencoder, DNNAE, SalTransformer, SalAE, SalSCINet, SalGATSCINet, SalGATSCINetV2, SalGATConvLSTM, SalGATConvGRU, ConvGRU, SalGATConvGRUwoSal, SalConvGRUwoALL\n",
    "\n",
    "\n",
    "import os, random\n",
    "import torch\n",
    "from torch.nn import TransformerAE\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "# from args import get_parser\n",
    "\n",
    "\n",
    "def load_and_save(category, filename, dataset, dataset_folder, output_folder):\n",
    "    temp = np.genfromtxt(\n",
    "        path.join(dataset_folder, category, filename),\n",
    "        dtype=np.float32,\n",
    "        delimiter=\",\",\n",
    "    )\n",
    "    print(dataset, category, filename, temp.shape)\n",
    "    with open(path.join(output_folder, dataset + \"_\" + category + \".pkl\"), \"wb\") as file:\n",
    "        dump(temp, file)\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "    \"\"\" Method from OmniAnomaly (https://github.com/NetManAIOps/OmniAnomaly) \"\"\"\n",
    "\n",
    "    if dataset == \"SMD\":\n",
    "        dataset_folder = \"/home/sangyup/saliency_anomaly_detection/dataset/ServerMachineDataset\"\n",
    "        output_folder = \"/home/sangyup/saliency_anomaly_detection/dataset/ServerMachineDataset/processed\"\n",
    "        makedirs(output_folder, exist_ok=True)\n",
    "        file_list = listdir(path.join(dataset_folder, \"train\"))\n",
    "        for filename in file_list:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                load_and_save(\n",
    "                    \"train\",\n",
    "                    filename,\n",
    "                    filename.strip(\".txt\"),\n",
    "                    dataset_folder,\n",
    "                    output_folder,\n",
    "                )\n",
    "                load_and_save(\n",
    "                    \"test_label\",\n",
    "                    filename,\n",
    "                    filename.strip(\".txt\"),\n",
    "                    dataset_folder,\n",
    "                    output_folder,\n",
    "                )\n",
    "                load_and_save(\n",
    "                    \"test\",\n",
    "                    filename,\n",
    "                    filename.strip(\".txt\"),\n",
    "                    dataset_folder,\n",
    "                    output_folder,\n",
    "                )\n",
    "\n",
    "    elif dataset == \"SMAP\" or dataset == \"MSL\":\n",
    "        dataset_folder = \"/home/sangyup/saliency_anomaly_detection/dataset/SMAPMSL/data\"\n",
    "        output_folder = \"/home/sangyup/saliency_anomaly_detection/dataset/SMAPMSL/data/processed\"\n",
    "        makedirs(output_folder, exist_ok=True)\n",
    "        with open(path.join(dataset_folder, \"labeled_anomalies.csv\"), \"r\") as file:\n",
    "            csv_reader = reader(file, delimiter=\",\")\n",
    "            res = [row for row in csv_reader][1:]\n",
    "        res = sorted(res, key=lambda k: k[0])\n",
    "        data_info = [row for row in res if row[1] == dataset and row[0] != \"P-2\"]\n",
    "        labels = []\n",
    "        for row in data_info:\n",
    "            anomalies = literal_eval(row[2])\n",
    "            length = int(row[-1])\n",
    "            label = np.zeros([length], dtype=np.bool_)\n",
    "            for anomaly in anomalies:\n",
    "                label[anomaly[0] : anomaly[1] + 1] = True\n",
    "            labels.extend(label)\n",
    "\n",
    "        labels = np.asarray(labels)\n",
    "        print(dataset, \"test_label\", labels.shape)\n",
    "\n",
    "        with open(path.join(output_folder, dataset + \"_\" + \"test_label\" + \".pkl\"), \"wb\") as file:\n",
    "            dump(labels, file)\n",
    "\n",
    "        def concatenate_and_save(category):\n",
    "            data = []\n",
    "            for row in data_info:\n",
    "                filename = row[0]\n",
    "                temp = np.load(path.join(dataset_folder, category, filename + \".npy\"))\n",
    "                data.extend(temp)\n",
    "            data = np.asarray(data)\n",
    "            print(dataset, category, data.shape)\n",
    "            with open(path.join(output_folder, dataset + \"_\" + category + \".pkl\"), \"wb\") as file:\n",
    "                dump(data, file)\n",
    "\n",
    "        for c in [\"train\", \"test\"]:\n",
    "            concatenate_and_save(c)\n",
    "\n",
    "def normalize_data(data, scaler=None):\n",
    "    data = np.asarray(data, dtype=np.float32)\n",
    "    if np.any(sum(np.isnan(data))):\n",
    "        data = np.nan_to_num(data)\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    print(\"Data normalized\")\n",
    "\n",
    "    return data, scaler\n",
    "\n",
    "def get_data_dim(dataset):\n",
    "    \"\"\"\n",
    "    :param dataset: Name of dataset\n",
    "    :return: Number of dimensions in data\n",
    "    \"\"\"\n",
    "    if dataset == \"SMAP\":\n",
    "        return 25\n",
    "    elif dataset == \"MSL\":\n",
    "        return 55\n",
    "    elif str(dataset).startswith(\"machine\"):\n",
    "        return 38\n",
    "    else:\n",
    "        raise ValueError(\"unknown dataset \" + str(dataset))\n",
    "\n",
    "        \n",
    "def get_data(dataset, max_train_size=None, max_test_size=None,\n",
    "             normalize=False, spec_res=False, train_start=0, test_start=0):\n",
    "    \"\"\"\n",
    "    Get data from pkl files\n",
    "\n",
    "    return shape: (([train_size, x_dim], [train_size] or None), ([test_size, x_dim], [test_size]))\n",
    "    Method from OmniAnomaly (https://github.com/NetManAIOps/OmniAnomaly)\n",
    "    \"\"\"\n",
    "    prefix = \"/home/sangyup/saliency_anomaly_detection/dataset\"\n",
    "    if str(dataset).startswith(\"machine\"):\n",
    "        prefix += \"/ServerMachineDataset/processed\"\n",
    "    elif dataset in [\"MSL\", \"SMAP\"]:\n",
    "        prefix += \"/SMAPMSL/data/processed\"\n",
    "    if max_train_size is None:\n",
    "        train_end = None\n",
    "    else:\n",
    "        train_end = train_start + max_train_size\n",
    "    if max_test_size is None:\n",
    "        test_end = None\n",
    "    else:\n",
    "        test_end = test_start + max_test_size\n",
    "    print(\"load data of:\", dataset)\n",
    "    print(\"train: \", train_start, train_end)\n",
    "    print(\"test: \", test_start, test_end)\n",
    "    x_dim = get_data_dim(dataset)\n",
    "    f = open(os.path.join(prefix, dataset + \"_train.pkl\"), \"rb\")\n",
    "    train_data = pickle.load(f).reshape((-1, x_dim))[train_start:train_end, :]\n",
    "    f.close()\n",
    "    try:\n",
    "        f = open(os.path.join(prefix, dataset + \"_test.pkl\"), \"rb\")\n",
    "        test_data = pickle.load(f).reshape((-1, x_dim))[test_start:test_end, :]\n",
    "        f.close()\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        test_data = None\n",
    "    try:\n",
    "        f = open(os.path.join(prefix, dataset + \"_test_label.pkl\"), \"rb\")\n",
    "        test_label = pickle.load(f).reshape((-1))[test_start:test_end]\n",
    "        f.close()\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        test_label = None\n",
    "\n",
    "    if normalize:\n",
    "        train_data, scaler = normalize_data(train_data, scaler=None)\n",
    "        test_data, _ = normalize_data(test_data, scaler=scaler)\n",
    "\n",
    "    print(\"train set shape: \", train_data.shape)\n",
    "    print(\"test set shape: \", test_data.shape)\n",
    "    print(\"test set label shape: \", None if test_label is None else test_label.shape)\n",
    "    return (train_data, None), (test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSL test_label (73729,)\n",
      "MSL train (58317, 55)\n",
      "MSL test (73729, 55)\n"
     ]
    }
   ],
   "source": [
    "ds = 'MSL'\n",
    "load_data(ds.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data of: MSL\n",
      "train:  0 None\n",
      "test:  0 None\n",
      "Data normalized\n",
      "Data normalized\n",
      "train set shape:  (58317, 55)\n",
      "test set shape:  (73729, 55)\n",
      "test set label shape:  (73729,)\n"
     ]
    }
   ],
   "source": [
    "# SMD 1-1, 1-2, 1-3\n",
    "# (x_train, _), (x_test, y_test) = get_data('machine-1-1', normalize=True)\n",
    "\n",
    "# MSL\n",
    "# (x_train, _), (x_test, y_test) = get_data('MSL', normalize=True)\n",
    "\n",
    "# SMAP\n",
    "(x_train, _), (x_test, y_test) = get_data('SMAP', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73629, 100, 55)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_series(series, n_past, n_future):\n",
    "    '''\n",
    "\n",
    "    :param series: input time series\n",
    "    :param n_past: number of past observations\n",
    "    :param n_future: number of future series\n",
    "    :return: X, y(label)\n",
    "    '''\n",
    "    X, y = list(), list()\n",
    "    for window_start in range(len(series)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(series):\n",
    "            break\n",
    "        # slicing the past and future parts of the window\n",
    "        past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
    "        X.append(past)\n",
    "        y.append(future)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "x_test_windowed, _ = split_series(x_test, 100, 1)\n",
    "np.array(x_test_windowed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ReconDataset(Dataset):\n",
    "    def __init__(self, data, window, target_cols):\n",
    "        self.data = torch.Tensor(data)\n",
    "        self.window = window\n",
    "        self.target_cols = target_cols\n",
    "        self.shape = self.__getshape__()\n",
    "        self.size = self.__getsize__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window]\n",
    "        y = self.data[index:index+self.window]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) -  self.window \n",
    "    \n",
    "    def __getshape__(self):\n",
    "        return (self.__len__(), *self.__getitem__(0)[0].shape)\n",
    "\n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())\n",
    "\n",
    "class ForecastDataset(Dataset):\n",
    "    def __init__(self, data, window, target_cols):\n",
    "        self.data = torch.Tensor(data)\n",
    "        self.window = window\n",
    "        self.target_cols = target_cols\n",
    "        self.shape = self.__getshape__()\n",
    "        self.size = self.__getsize__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window]\n",
    "        y = self.data[index+self.window,0:self.target_cols]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) -  self.window \n",
    "    \n",
    "    def __getshape__(self):\n",
    "        return (self.__len__(), *self.__getitem__(0)[0].shape)\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())\n",
    "\n",
    "class ReconForecastDataset(Dataset):\n",
    "    def __init__(self, data, window, horizon):\n",
    "        self.data = torch.Tensor(data)\n",
    "        self.window = window\n",
    "        self.horizon = horizon\n",
    "        self.shape = self.__getshape__()\n",
    "        self.size = self.__getsize__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window]\n",
    "        y_recon = self.data[index:index+self.window]\n",
    "        y_fore = self.data[index+self.window:index+self.window+self.horizon]\n",
    "        return x, y_recon, y_fore\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) -  self.window \n",
    "    \n",
    "    def __getshape__(self):\n",
    "        return (self.__len__(), *self.__getitem__(0)[0].shape), (self.__len__(), *self.__getitem__(0)[1].shape), (self.__len__(), *self.__getitem__(0)[2].shape)\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())\n",
    "\n",
    "\n",
    "def create_data_loaders(train_dataset, batch_size, val_split=0.1, shuffle=False, test_dataset=None):\n",
    "    train_loader, val_loader, test_loader = None, None, None\n",
    "    if val_split == 0.0:\n",
    "        print(f\"train_size: {len(train_dataset)}\")\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "\n",
    "    else:\n",
    "        dataset_size = len(train_dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(val_split * dataset_size))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, sampler=train_sampler, drop_last=True)\n",
    "        val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, sampler=valid_sampler, drop_last=True)\n",
    "\n",
    "        print(f\"train_size: {len(train_indices)}\")\n",
    "        print(f\"validation_size: {len(val_indices)}\")\n",
    "\n",
    "    if test_dataset is not None:\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        print(f\"test_size: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# From Informer\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self):\n",
    "        self.mean = 0.\n",
    "        self.std = 1.\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.mean = data.mean(0)\n",
    "        self.std = data.std(0)\n",
    "\n",
    "    def transform(self, data):\n",
    "        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean\n",
    "        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std\n",
    "        return (data - mean) / std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean\n",
    "        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std\n",
    "        return (data * std) + mean\n",
    "\n",
    "# 시간 특징을 freq에 따라 추출\n",
    "def time_features(dates, freq='h'):\n",
    "    dates['month'] = dates.date.apply(lambda row:row.month,1)\n",
    "    dates['day'] = dates.date.apply(lambda row:row.day,1)\n",
    "    dates['weekday'] = dates.date.apply(lambda row:row.weekday(),1)\n",
    "    dates['hour'] = dates.date.apply(lambda row:row.hour,1)\n",
    "    dates['minute'] = dates.date.apply(lambda row:row.minute,1)\n",
    "    dates['minute'] = dates.minute.map(lambda x:x//15)\n",
    "    freq_map = {\n",
    "        'y':[],'m':['month'],'w':['month'],'d':['month','day','weekday'],\n",
    "        'b':['month','day','weekday'],'h':['month','day','weekday','hour'],\n",
    "        't':['month','day','weekday','hour','minute'],\n",
    "    }\n",
    "    return dates[freq_map[freq.lower()]].values\n",
    "\n",
    "# 한번의 batch를 실행하는 코드\n",
    "def _process_one_batch(batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "    batch_x = batch_x.float().to(device)\n",
    "    batch_y = batch_y.float()\n",
    "    batch_x_mark = batch_x_mark.float().to(device)\n",
    "    batch_y_mark = batch_y_mark.float().to(device)\n",
    "    dec_inp = torch.zeros([batch_y.shape[0], pred_len, batch_y.shape[-1]]).float()\n",
    "    dec_inp = torch.cat([batch_y[:,:label_len,:], dec_inp], dim=1).float().to(device)\n",
    "    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "    batch_y = batch_y[:,-pred_len:,0:].to(device)\n",
    "    return outputs, batch_y\n",
    "\n",
    "\n",
    "class Dataset_Pred(Dataset):\n",
    "    def __init__(self, dataframe, size=None, scale=True):\n",
    "        self.seq_len = size[0]\n",
    "        self.label_len = size[1]\n",
    "        self.pred_len = size[2]\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.scale = scale\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = self.dataframe\n",
    "        df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"])\n",
    "\n",
    "        delta = df_raw[\"date\"].iloc[1] - df_raw[\"date\"].iloc[0]\n",
    "        if delta>=timedelta(hours=1):\n",
    "            self.freq='h'\n",
    "        else:\n",
    "            self.freq='t'\n",
    "\n",
    "        border1 = 0\n",
    "        border2 = len(df_raw)\n",
    "        cols_data = df_raw.columns[1:]\n",
    "        df_data = df_raw[cols_data]\n",
    "\n",
    "\n",
    "        if self.scale:\n",
    "            self.scaler.fit(df_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "            \n",
    "        tmp_stamp = df_raw[['date']][border1:border2]\n",
    "        tmp_stamp['date'] = pd.to_datetime(tmp_stamp.date)\n",
    "        pred_dates = pd.date_range(tmp_stamp.date.values[-1], periods=self.pred_len+1, freq=self.freq)\n",
    "        \n",
    "        df_stamp = pd.DataFrame(columns = ['date'])\n",
    "        df_stamp.date = list(tmp_stamp.date.values) + list(pred_dates[1:])\n",
    "        data_stamp = time_features(df_stamp, freq=self.freq)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len- self.pred_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 40745\n",
      "validation_size: 17462\n",
      "test_size: 73629\n"
     ]
    }
   ],
   "source": [
    "window_size = 100\n",
    "batch_size = 512\n",
    "horizon = 10\n",
    "\n",
    "train_dataset = ReconForecastDataset(x_train, window_size, horizon)\n",
    "indices = torch.arange(len(train_dataset)-horizon)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
    "test_dataset = ReconForecastDataset(x_test, window_size, horizon)\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_dataset, batch_size, val_split=0.3, shuffle=False, test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'SalGATConvGRUwoAll_saliency_MSL'\n",
    "with open(\"result/model_\" + model_name + \"_best.pt\", \"rb\") as f:\n",
    "    SAVED_MODEL = torch.load(f)\n",
    "\n",
    "print(SAVED_MODEL['best_epoch'])\n",
    "\n",
    "MODEL = SalGATConvGRU(seq_len=window_size, output_len=10, n_features=x_train.shape[1], out_n_features=x_train.shape[1], embedding_dim=int(x_train.shape[1]/2), kernel_size=3, cell='gru')\n",
    "# MODEL = SalGATConvGRUwoSal(seq_len=window_size, output_len=10, n_features=x_train.shape[1], out_n_features=x_train.shape[1], embedding_dim=int(x_train.shape[1]/2), kernel_size=3, cell='gru')\n",
    "# MODEL = SalConvGRUwoALL(seq_len=window_size, output_len=10, n_features=x_train.shape[1], out_n_features=x_train.shape[1], embedding_dim=int(x_train.shape[1]/2), kernel_size=3, cell='gru')\n",
    "\n",
    "MODEL.cuda()    \n",
    "MODEL.load_state_dict(SAVED_MODEL[\"state\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference_SAL(dataloader, model, batch_size, TF_alpha):\n",
    "    dist, dist_sal, fin_dist1, fin_dist2, guess, sal_list = [], [], [], [], [], []\n",
    "    mse = torch.nn.MSELoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x,y_recon,y_fore) in enumerate(dataloader):\n",
    "            x = x.cuda()\n",
    "            y_recon = y_recon.cuda()\n",
    "            y_fore = y_fore.cuda()\n",
    "            \n",
    "            sal, dec_x, out = model(x)\n",
    "            \n",
    "            for y_r, y_f, d, o in zip(y_recon, y_fore, dec_x, out):\n",
    "                d_s = torch.sum(torch.square(y_r - d)).cpu().numpy()\n",
    "                d_o = torch.sum(torch.square(y_f - o)).cpu().numpy()\n",
    "                # d_s = torch.mean(torch.square(y_r - d)).cpu().numpy()\n",
    "                # d_o = torch.mean(torch.square(y_f - o)).cpu().numpy()\n",
    "                \n",
    "                dist_sal.append(d_s)\n",
    "                dist.append(d_o)\n",
    "\n",
    "                # inference_score1 = (d_s + d_o)/(1+TF_alpha)\n",
    "                inference_score1 = ((1-TF_alpha)*d_s + TF_alpha*d_o)/(1+TF_alpha)\n",
    "                inference_score2 = (d_s + TF_alpha*d_o)/(1+TF_alpha)\n",
    "                fin_dist1.append(inference_score1)\n",
    "                fin_dist2.append(inference_score2)\n",
    "                # fin_dist.append((1-TF_alpha)*mse(y_s,d).item() + TF_alpha*mse(y_s, o).item())\n",
    "                # fin_dist.append((TF_alpha*torch.cdist(y_s, o, p=2.0) + (1-TF_alpha)*torch.cdist(y_s, d, p=2.0)).cpu().numpy())\n",
    "\n",
    "            guess.append(out.cpu().numpy())\n",
    "            sal_list.append(sal.cpu().numpy())\n",
    "        \n",
    "            \n",
    "    return (\n",
    "        dist,\n",
    "        dist_sal,\n",
    "        fin_dist1,\n",
    "        fin_dist2,\n",
    "        np.concatenate(guess),\n",
    "        np.concatenate(sal_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 1s, sys: 27.2 s, total: 15min 28s\n",
      "Wall time: 23.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6021214114845479"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import metrics\n",
    "\n",
    "MODEL.eval()\n",
    "alpha = 1.0\n",
    "DIST, DIST_SAL, FIN_DIST1, FIN_DIST2, GUESS, SAL_LIST = inference_SAL(test_loader, MODEL, batch_size, TF_alpha=alpha)\n",
    "DIST_train, DIST_SAL_train, FIN_DIST1_train, FIN_DIST2_train, GUESS_train, SAL_LIST_train = inference_SAL(train_loader, MODEL, batch_size, TF_alpha=alpha)\n",
    "\n",
    "# anomaly score of test dataset\n",
    "res = SAL_LIST[:,-1:,:]\n",
    "res = res.reshape(res.shape[0], res.shape[2])\n",
    "res_mean = np.mean(res, axis=1)\n",
    "FD1wSAL = np.array(FIN_DIST1)*res_mean\n",
    "\n",
    "# anomaly score of train dataset --> to calculate the threshold\n",
    "res_train = SAL_LIST_train[:,-1:,:]\n",
    "res_train = res_train.reshape(res_train.shape[0], res_train.shape[2])\n",
    "res_mean_train = np.mean(res_train, axis=1)\n",
    "FD1wSAL_train = np.array(FIN_DIST1_train)*res_mean_train\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test[-len(FD1wSAL):].astype(int), FD1wSAL, pos_label=1)\n",
    "metrics.auc(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saliency",
   "language": "python",
   "name": "saliency"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
