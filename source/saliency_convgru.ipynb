{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from csv import reader\n",
    "from os import listdir, makedirs, path\n",
    "from pickle import dump\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from model import Seq2Seq, RecurrentAutoencoder, DNNAE, SalTransformer, SalAE, SalSCINet, SalGATSCINet, SalGATSCINetV2, SalGATConvLSTM, SalGATConvGRU, ConvGRU, SalGATConvGRUwoSal, SalConvGRUwoALL, SalGATConvGRUwoGAT\n",
    "\n",
    "\n",
    "import os, random\n",
    "import torch\n",
    "from torch.nn import TransformerAE\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "\n",
    "\n",
    "def load_and_save(category, filename, dataset, dataset_folder, output_folder):\n",
    "    temp = np.genfromtxt(\n",
    "        path.join(dataset_folder, category, filename),\n",
    "        dtype=np.float32,\n",
    "        delimiter=\",\",\n",
    "    )\n",
    "    print(dataset, category, filename, temp.shape)\n",
    "    with open(path.join(output_folder, dataset + \"_\" + category + \".pkl\"), \"wb\") as file:\n",
    "        dump(temp, file)\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "    \"\"\" Method from OmniAnomaly (https://github.com/NetManAIOps/OmniAnomaly) \"\"\"\n",
    "\n",
    "    if dataset == \"SMD\":\n",
    "        dataset_folder = \"/home/sangyup/saliency_anomaly_detection/dataset/ServerMachineDataset\"\n",
    "        output_folder = \"/home/sangyup/saliency_anomaly_detection/dataset/ServerMachineDataset/processed\"\n",
    "        makedirs(output_folder, exist_ok=True)\n",
    "        file_list = listdir(path.join(dataset_folder, \"train\"))\n",
    "        for filename in file_list:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                load_and_save(\n",
    "                    \"train\",\n",
    "                    filename,\n",
    "                    filename.strip(\".txt\"),\n",
    "                    dataset_folder,\n",
    "                    output_folder,\n",
    "                )\n",
    "                load_and_save(\n",
    "                    \"test_label\",\n",
    "                    filename,\n",
    "                    filename.strip(\".txt\"),\n",
    "                    dataset_folder,\n",
    "                    output_folder,\n",
    "                )\n",
    "                load_and_save(\n",
    "                    \"test\",\n",
    "                    filename,\n",
    "                    filename.strip(\".txt\"),\n",
    "                    dataset_folder,\n",
    "                    output_folder,\n",
    "                )\n",
    "\n",
    "    elif dataset == \"SMAP\" or dataset == \"MSL\":\n",
    "        # Change the dataset path accordingly\n",
    "        dataset_folder = \"/SMAPMSL/data\"\n",
    "        output_folder = \"/SMAPMSL/data/processed\"\n",
    "        makedirs(output_folder, exist_ok=True)\n",
    "        with open(path.join(dataset_folder, \"labeled_anomalies.csv\"), \"r\") as file:\n",
    "            csv_reader = reader(file, delimiter=\",\")\n",
    "            res = [row for row in csv_reader][1:]\n",
    "        res = sorted(res, key=lambda k: k[0])\n",
    "        data_info = [row for row in res if row[1] == dataset and row[0] != \"P-2\"]\n",
    "        labels = []\n",
    "        for row in data_info:\n",
    "            anomalies = literal_eval(row[2])\n",
    "            length = int(row[-1])\n",
    "            label = np.zeros([length], dtype=np.bool_)\n",
    "            for anomaly in anomalies:\n",
    "                label[anomaly[0] : anomaly[1] + 1] = True\n",
    "            labels.extend(label)\n",
    "\n",
    "        labels = np.asarray(labels)\n",
    "        print(dataset, \"test_label\", labels.shape)\n",
    "\n",
    "        with open(path.join(output_folder, dataset + \"_\" + \"test_label\" + \".pkl\"), \"wb\") as file:\n",
    "            dump(labels, file)\n",
    "\n",
    "        def concatenate_and_save(category):\n",
    "            data = []\n",
    "            for row in data_info:\n",
    "                filename = row[0]\n",
    "                temp = np.load(path.join(dataset_folder, category, filename + \".npy\"))\n",
    "                data.extend(temp)\n",
    "            data = np.asarray(data)\n",
    "            print(dataset, category, data.shape)\n",
    "            with open(path.join(output_folder, dataset + \"_\" + category + \".pkl\"), \"wb\") as file:\n",
    "                dump(data, file)\n",
    "\n",
    "        for c in [\"train\", \"test\"]:\n",
    "            concatenate_and_save(c)\n",
    "\n",
    "def normalize_data(data, scaler=None):\n",
    "    data = np.asarray(data, dtype=np.float32)\n",
    "    if np.any(sum(np.isnan(data))):\n",
    "        data = np.nan_to_num(data)\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    print(\"Data normalized\")\n",
    "\n",
    "    return data, scaler\n",
    "\n",
    "def get_data_dim(dataset):\n",
    "    \"\"\"\n",
    "    :param dataset: Name of dataset\n",
    "    :return: Number of dimensions in data\n",
    "    \"\"\"\n",
    "    if dataset == \"SMAP\":\n",
    "        return 25\n",
    "    elif dataset == \"MSL\":\n",
    "        return 55\n",
    "    elif str(dataset).startswith(\"machine\"):\n",
    "        return 38\n",
    "    else:\n",
    "        raise ValueError(\"unknown dataset \" + str(dataset))\n",
    "\n",
    "        \n",
    "def get_data(dataset, max_train_size=None, max_test_size=None,\n",
    "             normalize=False, spec_res=False, train_start=0, test_start=0):\n",
    "    \"\"\"\n",
    "    Get data from pkl files\n",
    "\n",
    "    return shape: (([train_size, x_dim], [train_size] or None), ([test_size, x_dim], [test_size]))\n",
    "    Method from OmniAnomaly (https://github.com/NetManAIOps/OmniAnomaly)\n",
    "    \"\"\"\n",
    "    prefix = \"/home/sangyup/saliency_anomaly_detection/dataset\"\n",
    "    if str(dataset).startswith(\"machine\"):\n",
    "        prefix += \"/ServerMachineDataset/processed\"\n",
    "    elif dataset in [\"MSL\", \"SMAP\"]:\n",
    "        prefix += \"/SMAPMSL/data/processed\"\n",
    "    if max_train_size is None:\n",
    "        train_end = None\n",
    "    else:\n",
    "        train_end = train_start + max_train_size\n",
    "    if max_test_size is None:\n",
    "        test_end = None\n",
    "    else:\n",
    "        test_end = test_start + max_test_size\n",
    "    print(\"load data of:\", dataset)\n",
    "    print(\"train: \", train_start, train_end)\n",
    "    print(\"test: \", test_start, test_end)\n",
    "    x_dim = get_data_dim(dataset)\n",
    "    f = open(os.path.join(prefix, dataset + \"_train.pkl\"), \"rb\")\n",
    "    train_data = pickle.load(f).reshape((-1, x_dim))[train_start:train_end, :]\n",
    "    f.close()\n",
    "    try:\n",
    "        f = open(os.path.join(prefix, dataset + \"_test.pkl\"), \"rb\")\n",
    "        test_data = pickle.load(f).reshape((-1, x_dim))[test_start:test_end, :]\n",
    "        f.close()\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        test_data = None\n",
    "    try:\n",
    "        f = open(os.path.join(prefix, dataset + \"_test_label.pkl\"), \"rb\")\n",
    "        test_label = pickle.load(f).reshape((-1))[test_start:test_end]\n",
    "        f.close()\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        test_label = None\n",
    "\n",
    "    if normalize:\n",
    "        train_data, scaler = normalize_data(train_data, scaler=None)\n",
    "        test_data, _ = normalize_data(test_data, scaler=scaler)\n",
    "\n",
    "    print(\"train set shape: \", train_data.shape)\n",
    "    print(\"test set shape: \", test_data.shape)\n",
    "    print(\"test set label shape: \", None if test_label is None else test_label.shape)\n",
    "    return (train_data, None), (test_data, test_label)\n",
    "\n",
    "\n",
    "def split_series(series, n_past, n_future):\n",
    "    '''\n",
    "\n",
    "    :param series: input time series\n",
    "    :param n_past: number of past observations\n",
    "    :param n_future: number of future series\n",
    "    :return: X, y(label)\n",
    "    '''\n",
    "    X, y = list(), list()\n",
    "    for window_start in range(len(series)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(series):\n",
    "            break\n",
    "        # slicing the past and future parts of the window\n",
    "        past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
    "        X.append(past)\n",
    "        y.append(future)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data of: MSL\n",
      "train:  0 None\n",
      "test:  0 None\n",
      "Data normalized\n",
      "Data normalized\n",
      "train set shape:  (58317, 55)\n",
      "test set shape:  (73729, 55)\n",
      "test set label shape:  (73729,)\n"
     ]
    }
   ],
   "source": [
    "ds = 'MSL'  # SMD / MSL / SMD\n",
    "load_data(ds.upper())\n",
    "\n",
    "# SMD 1-1, 1-2, 1-3\n",
    "# (x_train, _), (x_test, y_test) = get_data('machine-1-1', normalize=True)\n",
    "\n",
    "# MSL / SMAP\n",
    "# (x_train, _), (x_test, y_test) = get_data(ds, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ReconDataset(Dataset):\n",
    "    def __init__(self, data, window, target_cols):\n",
    "        self.data = torch.Tensor(data)\n",
    "        self.window = window\n",
    "        self.target_cols = target_cols\n",
    "        self.shape = self.__getshape__()\n",
    "        self.size = self.__getsize__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window]\n",
    "        y = self.data[index:index+self.window]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) -  self.window \n",
    "    \n",
    "    def __getshape__(self):\n",
    "        return (self.__len__(), *self.__getitem__(0)[0].shape)\n",
    "\n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())\n",
    "\n",
    "class ForecastDataset(Dataset):\n",
    "    def __init__(self, data, window, target_cols):\n",
    "        self.data = torch.Tensor(data)\n",
    "        self.window = window\n",
    "        self.target_cols = target_cols\n",
    "        self.shape = self.__getshape__()\n",
    "        self.size = self.__getsize__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window]\n",
    "        y = self.data[index+self.window,0:self.target_cols]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) -  self.window \n",
    "    \n",
    "    def __getshape__(self):\n",
    "        return (self.__len__(), *self.__getitem__(0)[0].shape)\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())\n",
    "\n",
    "class ReconForecastDataset(Dataset):\n",
    "    def __init__(self, data, window, horizon):\n",
    "        self.data = torch.Tensor(data)\n",
    "        self.window = window\n",
    "        self.horizon = horizon\n",
    "        self.shape = self.__getshape__()\n",
    "        self.size = self.__getsize__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window]\n",
    "        y_recon = self.data[index:index+self.window]\n",
    "        y_fore = self.data[index+self.window:index+self.window+self.horizon]\n",
    "        return x, y_recon, y_fore\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) -  self.window \n",
    "    \n",
    "    def __getshape__(self):\n",
    "        return (self.__len__(), *self.__getitem__(0)[0].shape), (self.__len__(), *self.__getitem__(0)[1].shape), (self.__len__(), *self.__getitem__(0)[2].shape)\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())\n",
    "\n",
    "\n",
    "def create_data_loaders(train_dataset, batch_size, val_split=0.1, shuffle=False, test_dataset=None):\n",
    "    train_loader, val_loader, test_loader = None, None, None\n",
    "    if val_split == 0.0:\n",
    "        print(f\"train_size: {len(train_dataset)}\")\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "\n",
    "    else:\n",
    "        dataset_size = len(train_dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(val_split * dataset_size))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, sampler=train_sampler, drop_last=True)\n",
    "        val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, sampler=valid_sampler, drop_last=True)\n",
    "\n",
    "        print(f\"train_size: {len(train_indices)}\")\n",
    "        print(f\"validation_size: {len(val_indices)}\")\n",
    "\n",
    "    if test_dataset is not None:\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "        print(f\"test_size: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: 40745\n",
      "validation_size: 17462\n",
      "test_size: 73629\n"
     ]
    }
   ],
   "source": [
    "window_size = 100   # Historical data lookback\n",
    "batch_size = 512    # Batch size\n",
    "horizon = 10        # Future prediction horizon length\n",
    "\n",
    "train_dataset = ReconForecastDataset(x_train, window_size, horizon)\n",
    "indices = torch.arange(len(train_dataset)-horizon)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
    "test_dataset = ReconForecastDataset(x_test, window_size, horizon)\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_dataset, batch_size, val_split=0.3, shuffle=False, test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def train_model(dataloader, val_loader, model, batch_size, n_epochs, model_name):\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    \n",
    "    loss_fn_AE = torch.nn.MSELoss()\n",
    "    loss_fn_sci = torch.nn.MSELoss()\n",
    "\n",
    "    epochs = range(n_epochs)\n",
    "    best = {\"loss\": sys.float_info.max}\n",
    "    loss_history, loss_history_AE, loss_history_TF = [], [], []\n",
    "    val_loss_history, val_loss_history_AE, val_loss_history_TF = [], [], []\n",
    "    for e in epochs:\n",
    "        model.train()\n",
    "        start = time()\n",
    "        epoch_loss = 0\n",
    "        epoch_loss_AE = 0\n",
    "        epoch_loss_sci = 0\n",
    "        for i, (x,y_recon,y_fore) in enumerate(dataloader):  \n",
    "            optimizer.zero_grad()\n",
    "            x = x.cuda()\n",
    "            y_recon = y_recon.cuda()\n",
    "            y_fore = y_fore.cuda()\n",
    "            sal, dec_x, out = model(x)\n",
    "            \n",
    "            loss_AE = loss_fn_AE(y_recon, dec_x)\n",
    "            loss_sci = loss_fn_sci(y_fore, out)\n",
    "            loss = loss_AE + loss_sci\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_loss_AE += loss_AE.item()\n",
    "            epoch_loss_sci += loss_sci.item()\n",
    "            optimizer.step()\n",
    "        loss_history.append(epoch_loss)\n",
    "        loss_history_AE.append(epoch_loss_AE)\n",
    "        loss_history_TF.append(epoch_loss_sci)\n",
    "\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_loss_AE = 0\n",
    "        val_epoch_loss_sci = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (x,y_recon,y_fore) in enumerate(val_loader):  \n",
    "                x = x.cuda()\n",
    "                y_recon = y_recon.cuda()\n",
    "                y_fore = y_fore.cuda()\n",
    "                sal, dec_x, out = model(x)\n",
    "\n",
    "                valloss_AE = loss_fn_AE(y_recon, dec_x)\n",
    "                valloss_sci = loss_fn_sci(y_fore, out)\n",
    "                valloss = valloss_AE + valloss_sci\n",
    "                \n",
    "                val_epoch_loss += valloss.item()\n",
    "                val_epoch_loss_AE += valloss_AE.item()\n",
    "                val_epoch_loss_sci += valloss_sci.item()\n",
    "                \n",
    "                val_loss_history.append(val_epoch_loss)\n",
    "                val_loss_history_AE.append(val_epoch_loss_AE)\n",
    "                val_loss_history_TF.append(val_epoch_loss_sci)  \n",
    "\n",
    "        print(f'Training loss EPOCH: [{e+1}|{len(epochs)}], training loss: [{epoch_loss}], AE loss: [{epoch_loss_AE}], TF loss: [{epoch_loss_sci}] took', time()-start)\n",
    "        print(f'Validation loss EPOCH: [{e+1}|{len(epochs)}], validation loss: [{val_epoch_loss}], AE loss: [{val_epoch_loss_AE}], TF loss: [{val_epoch_loss_sci}]')\n",
    "        if val_epoch_loss < best[\"loss\"]:\n",
    "            best[\"state\"] = model.state_dict()\n",
    "            best[\"loss\"] = val_epoch_loss\n",
    "            best[\"loss_AE\"] = val_epoch_loss_AE\n",
    "            best[\"loss_TF\"] = val_epoch_loss_sci\n",
    "            best[\"epoch\"] = e + 1\n",
    "            with open(\"result/model_\" + model_name + \"_best.pt\", \"wb\") as f:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"state\": best[\"state\"],\n",
    "                        \"best_epoch\": best[\"epoch\"],\n",
    "                        \"loss_history\": val_loss_history,\n",
    "                        \"loss_AE_history\": val_loss_history_AE,\n",
    "                        \"loss_TF_history\": val_loss_history_TF,\n",
    "                    },\n",
    "                    f,\n",
    "                )\n",
    "\n",
    "    # Save last epoch\n",
    "    with open(\"result/model_\" + model_name + \"_last.pt\", \"wb\") as f:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"state\": model.state_dict(),\n",
    "                \"best_epoch\": e + 1,\n",
    "                \"loss_history\": val_loss_history,\n",
    "                \"loss_AE_history\": val_loss_history_AE,\n",
    "                \"loss_TF_history\": val_loss_history_TF,\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "\n",
    "    return best, loss_history, loss_history_AE, loss_history_TF\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SalConvGRUwoALL(\n",
       "  (encoder): RecurEncoder(\n",
       "    (rnn1): GRU(55, 54, batch_first=True)\n",
       "    (rnn2): GRU(54, 27, batch_first=True)\n",
       "  )\n",
       "  (decoder): RecurDecoder(\n",
       "    (rnn1): GRU(27, 27, batch_first=True)\n",
       "    (rnn2): GRU(27, 54, batch_first=True)\n",
       "    (output_layer): Linear(in_features=54, out_features=55, bias=True)\n",
       "    (timedist): TimeDistributed(\n",
       "      (module): Linear(in_features=54, out_features=55, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_gat): FeatureAttentionLayer(\n",
       "    (lin): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (temporal_gat): TemporalAttentionLayer(\n",
       "    (lin): Linear(in_features=55, out_features=55, bias=True)\n",
       "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (convGRU): ConvGRU(\n",
       "    (cell_list): ModuleList(\n",
       "      (0): ConvGRUCell(\n",
       "        (conv_gates): Conv2d(87, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv_can): Conv2d(87, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): ConvGRUCell(\n",
       "        (conv_gates): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv_can): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=64, out_features=55, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MODEL = SalGATConvGRU(seq_len=window_size, output_len=10, n_features=x_train.shape[1], out_n_features=x_train.shape[1], embedding_dim=int(x_train.shape[1]/2), kernel_size=3, cell='gru')\n",
    "# MODEL = SalGATConvGRUwoSal(seq_len=window_size, output_len=10, n_features=x_train.shape[1], out_n_features=x_train.shape[1], embedding_dim=int(x_train.shape[1]/2), kernel_size=3, cell='gru')\n",
    "# MODEL = SalConvGRUwoALL(seq_len=window_size, output_len=10, n_features=x_train.shape[1], out_n_features=x_train.shape[1], embedding_dim=int(x_train.shape[1]/2), kernel_size=3, cell='gru')\n",
    "MODEL.cuda()\n",
    "MODEL.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SalAD_'+ds\n",
    "epochs = 1000\n",
    "BEST_MODEL, LOSS_HISTORY, LOSS_HISTORY_AE, LOSS_HISTORY_TF = train_model(train_loader, val_loader, MODEL, batch_size, epochs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SalAD_MSL'\n",
    "with open(\"result/model_\" + model_name + \"_best.pt\", \"rb\") as f:\n",
    "    SAVED_MODEL = torch.load(f)\n",
    "\n",
    "print(SAVED_MODEL['best_epoch'])\n",
    "\n",
    "\n",
    "MODEL = SalGATConvGRU(seq_len=window_size, output_len=10, n_features=x_train.shape[1], out_n_features=x_train.shape[1], embedding_dim=int(x_train.shape[1]/2), kernel_size=3, cell='gru')\n",
    "# MODEL = SalGATConvGRUwoSal(seq_len=window_size, output_len=10, n_features=x_train.shape[1], out_n_features=x_train.shape[1], embedding_dim=int(x_train.shape[1]/2), kernel_size=3, cell='gru')\n",
    "# MODEL = SalConvGRUwoALL(seq_len=window_size, output_len=10, n_features=x_train.shape[1], out_n_features=x_train.shape[1], embedding_dim=int(x_train.shape[1]/2), kernel_size=3, cell='gru')\n",
    "\n",
    "MODEL.cuda()    \n",
    "MODEL.load_state_dict(SAVED_MODEL[\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_SAL(dataloader, model, batch_size, TF_alpha):\n",
    "    dist, dist_sal, fin_dist1, fin_dist2, guess, sal_list = [], [], [], [], [], []\n",
    "    mse = torch.nn.MSELoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x,y_recon,y_fore) in enumerate(dataloader):\n",
    "            x = x.cuda()\n",
    "            y_recon = y_recon.cuda()\n",
    "            y_fore = y_fore.cuda()\n",
    "            \n",
    "            sal, dec_x, out = model(x)\n",
    "            \n",
    "            for y_r, y_f, d, o in zip(y_recon, y_fore, dec_x, out):\n",
    "                d_s = torch.sum(torch.square(y_r - d)).cpu().numpy()\n",
    "                d_o = torch.sum(torch.square(y_f - o)).cpu().numpy()\n",
    "                # d_s = torch.mean(torch.square(y_r - d)).cpu().numpy()\n",
    "                # d_o = torch.mean(torch.square(y_f - o)).cpu().numpy()\n",
    "                \n",
    "                dist_sal.append(d_s)\n",
    "                dist.append(d_o)\n",
    "\n",
    "                # inference_score1 = (d_s + d_o)/(1+TF_alpha)\n",
    "                inference_score1 = ((1-TF_alpha)*d_s + TF_alpha*d_o)/(1+TF_alpha)\n",
    "                inference_score2 = (d_s + TF_alpha*d_o)/(1+TF_alpha)\n",
    "                fin_dist1.append(inference_score1)\n",
    "                fin_dist2.append(inference_score2)\n",
    "                # fin_dist.append((1-TF_alpha)*mse(y_s,d).item() + TF_alpha*mse(y_s, o).item())\n",
    "                # fin_dist.append((TF_alpha*torch.cdist(y_s, o, p=2.0) + (1-TF_alpha)*torch.cdist(y_s, d, p=2.0)).cpu().numpy())\n",
    "\n",
    "            guess.append(out.cpu().numpy())\n",
    "            sal_list.append(sal.cpu().numpy())\n",
    "        \n",
    "            \n",
    "    return (\n",
    "        dist,\n",
    "        dist_sal,\n",
    "        fin_dist1,\n",
    "        fin_dist2,\n",
    "        np.concatenate(guess),\n",
    "        np.concatenate(sal_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 19s, sys: 33.4 s, total: 20min 52s\n",
      "Wall time: 29.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MODEL.eval()\n",
    "DIST, DIST_SAL, FIN_DIST1, FIN_DIST2, GUESS, SAL_LIST = inference_SAL(test_loader, MODEL, batch_size, TF_alpha=1.0)\n",
    "DIST_train, DIST_SAL_train, FIN_DIST1_train, FIN_DIST2_train, GUESS_train, SAL_LIST_train = inference_SAL(train_loader, MODEL, batch_size, TF_alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import metrics\n",
    "\n",
    "MODEL.eval()\n",
    "alpha = 1.0\n",
    "DIST, DIST_SAL, FIN_DIST1, FIN_DIST2, GUESS, SAL_LIST = inference_SAL(test_loader, MODEL, batch_size, TF_alpha=alpha)\n",
    "DIST_train, DIST_SAL_train, FIN_DIST1_train, FIN_DIST2_train, GUESS_train, SAL_LIST_train = inference_SAL(train_loader, MODEL, batch_size, TF_alpha=alpha)\n",
    "\n",
    "# anomaly score of test dataset\n",
    "res = SAL_LIST[:,-1:,:]\n",
    "res = res.reshape(res.shape[0], res.shape[2])\n",
    "res_mean = np.mean(res, axis=1)\n",
    "FD1wSAL = np.array(FIN_DIST1)*res_mean\n",
    "\n",
    "# anomaly score of train dataset --> to calculate the threshold\n",
    "res_train = SAL_LIST_train[:,-1:,:]\n",
    "res_train = res_train.reshape(res_train.shape[0], res_train.shape[2])\n",
    "res_mean_train = np.mean(res_train, axis=1)\n",
    "FD1wSAL_train = np.array(FIN_DIST1_train)*res_mean_train\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test[-len(FD1wSAL):].astype(int), FD1wSAL, pos_label=1)\n",
    "metrics.auc(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saliency",
   "language": "python",
   "name": "saliency"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
